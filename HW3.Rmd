---
title: "Homework 3"
author: "Audrey Bahr"
date: "4/20/21"
output:
  html_document:
    df_print: paged
---
```{r}
library(tidyverse)
library(cowplot)
library(broom)
set.seed(245234)
```

##CHALLENGE 1:
The comparative primate dataset we have used from Kamilar and Cooper has in it a large number of variables related to life history and body size. For this exercise, the end aim is to fit a simple linear regression model to predict weaning age (WeaningAge_d) measured in days from species’ brain size (Brain_Size_Species_Mean) measured in grams. Do the following for both weaning age ~ brain size and log(weaning age) ~ log(brain size).

Fit the regression model and, using {ggplot2}, produce a scatterplot with the fitted line superimposed upon the data. Append the the fitted model equation to your plot.

```{r}
# read in data
kc <- read_csv("https://raw.githubusercontent.com/difiore/ada-2021-datasets/main/KamilarAndCooperData.csv", col_names = TRUE)

# select variables for model
d <- kc %>% 
  select(WeaningAge_d, Brain_Size_Species_Mean) %>%
  na.omit(kc)

# linear model
model1 <- lm(WeaningAge_d~Brain_Size_Species_Mean, data=d)
summary(model1)

# make equation label (adapted from StackOverflow)
eq <- substitute(y == b %.% x + a, 
         list(a = format(unname(coef(model1)[1]), digits = 3),
              b = format(unname(coef(model1)[2]), digits = 3))) %>%
  as.expression() %>%
  as.character()

# plot data with regression line
g1 <- ggplot(d, aes(y=WeaningAge_d, x=Brain_Size_Species_Mean)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE, color = "black") +
  ggtitle("Weaning Age vs Mean Brain Size") +
  ylab("Weaning Age (days)") +
  xlab("Mean Brain Size (g)") +
  theme_minimal() +
  geom_text(y = 250, x = 425, label = eq, parse = TRUE )

g1
```

Identify and interpret the point estimate of the slope (β1), as well as the outcome of the test associated with the hypotheses  
H0:β1=0,HA: β1≠0. Also, find a 90% CI for the slope (β1) parameter.

Using your model, add lines for the 90% confidence and prediction interval bands on the plot, and add a legend to differentiate between the lines.

```{r}
confint(model1, level = .9)
```
*An increase in mean brain size by 1g suggests an increase in weaning age by 2.64 days. We reject the null hypothesis that a change in mean brain size has no effect on weaning age (t = 14.28, p < 0.001). We are 90% confident that the change in weaning age per one-gram increase in brain size is between 2.33 and 2.94 days.*

```{r}
# 90% confidence intervals
df <- augment(model1, se_fit = TRUE)

ci <- predict(model1,
  newdata = data.frame(Brain_Size_Species_Mean = d$Brain_Size_Species_Mean),
  interval = "confidence", level = .9
) %>% 
  data.frame() %>%
  mutate(Brain_Size_Species_Mean = df$Brain_Size_Species_Mean)

# 90% prediction intervals

pi <- predict(model1,
  newdata = data.frame(Brain_Size_Species_Mean = d$Brain_Size_Species_Mean),
  interval = "prediction", level = 0.90
) %>%
  data.frame() %>%
  mutate("Brain_Size_Species_Mean" = d$Brain_Size_Species_Mean)

# adding ci and pi lines to plot
g1 + 
  geom_line(
  data = ci, aes(x = Brain_Size_Species_Mean, y = lwr, color = "90% CI")) + 
  geom_line(
  data = ci, aes(x = Brain_Size_Species_Mean, y = upr, color = "90% CI")) +
  geom_line(data = pi, aes(x = Brain_Size_Species_Mean, y = lwr, color = "90% PI")) +
  geom_line(data = pi, aes(x = Brain_Size_Species_Mean, y = upr, color = "90% PI")) +
  scale_color_manual(name = "Line Color", values = c(`90% PI` = "purple", `90% CI` = "light green"))
```

Produce a point estimate and associated 90% prediction interval for the weaning age of a species whose brain weight is 750 gm. Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not?
```{r}
predict(model1,
  newdata = data.frame(Brain_Size_Species_Mean = 750),
  interval = "prediction", level = 0.90
) 
```
I would not trust this estimate because we do not have data for brain sizes that high. Most of the mean brain sizes are less than 200g, so extrapolating the relationship between the two variables out to 750g may be inaccurate.

Repeated for the log of the variables.
```{r}
# calculate log
d <- d %>% 
  mutate(logWA = log(WeaningAge_d), logBSSM = log(Brain_Size_Species_Mean))

# linear model
model2 <- lm(logWA ~ logBSSM, data=d)
summary(model2)
# make equation label
eq <- substitute(y == b %.% x + a, 
         list(a = format(unname(coef(model2)[1]), digits = 3),
              b = format(unname(coef(model2)[2]), digits = 3))) %>%
  as.expression() %>%
  as.character()

# plot data with regression line
g2 <- ggplot(d, aes(y=logWA, x=logBSSM)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE, color = "black") +
  ggtitle("Log of Mean Brain Size vs Log of Weaning Age") +
  ylab("Log of Weaning Age") +
  xlab("Log of Mean Brain Size") +
  theme_minimal() +
  geom_text(y = 4, x = 4.5, label = eq, parse = TRUE )

g2
```

```{r}
(CI <- confint(model2, level = 1 - 0.1))
```
*An increase in mean brain size by 1% suggests an increase in the log of weaning age by 0.571%. We reject the null hypothesis that a change in the log of mean brain size has no effect on the log of weaning age (t = 18.66, p < 0.001). We are 90% confident that the percent change in weaning age per percent increase in mean brain size is between 0.52% and 0.62%.*

```{r}
# 90% confidence intervals
df <- augment(model2, se_fit = TRUE)

ci <- predict(model2,
  newdata = data.frame(logBSSM = d$logBSSM),
  interval = "confidence", level = .9
) %>% 
  data.frame() %>%
  mutate(logBSSM = df$logBSSM)

# 90% prediction intervals

pi <- predict(model2,
  newdata = data.frame(logBSSM = d$logBSSM),
  interval = "prediction", level = 0.90
) %>%
  data.frame() %>%
  mutate("logBSSM" = d$logBSSM)

# adding ci and pi lines to plot
g2 + 
  geom_line(
  data = ci, aes(x = logBSSM, y = lwr, color = "90% CI")) + 
  geom_line(
  data = ci, aes(x = logBSSM, y = upr, color = "90% CI")) +
  geom_line(data = pi, aes(x = logBSSM, y = lwr, color = "90% PI")) +
  geom_line(data = pi, aes(x = logBSSM, y = upr, color = "90% PI")) +
  scale_color_manual(name = "Line Color", values = c(`90% PI` = "purple", `90% CI` = "light green"))
```

```{r}
predict(model2,
  newdata = data.frame(logBSSM = log(750)),
  interval = "prediction", level = 0.90
) 
```
*I would trust this estimate a bit more now that the data is scaled and we have more points close to x = log(750).*

Looking at your two models (i.e., untransformed versus log-log transformed), which do you think is better? Why?

*In general, I think the log-log transformed model is better than the untransformed one because the data are more normally distributed and have a more linear relationship, which improves the accuracy of the model.*

##CHALLENGE 2:
When we initially discussed the central limit theorem and confidence intervals, we showed how we could use bootstrapping to estimate standard errors and confidence intervals around certain parameter values, like the mean. Using bootstrapping, we can also do the same for estimating standard errors and CIs around regression parameters, such as β coefficients.

Using the “KamilarAndCooperData.csv” dataset, run a linear regression looking at log(MeanGroupSize) in relation to log(Body_mass_female_mean) and report your β coeffiecients (slope and intercept).
```{r}
# calculate log
d2 <- kc %>% 
  mutate(logMGS = log(MeanGroupSize), logBMFM = log(Body_mass_female_mean)) %>%
  select(logMGS, logBMFM) %>%
  na.omit(kc)

# linear model
model_c2 <- lm(logMGS ~ logBMFM, data=d2)

# beta coefficients
coef(model_c2)
```
*The beta coefficients are -1.78 for the intercept and 0.506 for log of mean female body mass.*

Then, use bootstrapping to sample from the dataset 1000 times with replacement, each time fitting the same model and calculating the appropriate coefficients. [The size of each sample should be equivalent to the total number of observations in the dataset.] This generates a bootstrap sampling distribution for each β coefficient. Plot a histogram of these sampling distributions for β0 and β1.
```{r}
b0 <- vector(length = 1000)
b1 <- vector(length = 1000)
for (i in 1:1000) {
  
  slice <- d2 %>%
    slice_sample(n = nrow(d2), replace = TRUE)
  
  lm <- lm(logMGS ~ logBMFM, data=slice)
  
  b0[i] <- coef(lm)[[1]]
  b1[i] <- coef(lm)[[2]]
  
}

ggplot() +
  aes(b0) +
  geom_histogram(bins = 25) +
  theme_half_open() +
  ggtitle(expression("Bootstrap Distribution for" ~ beta[0])) +
  xlab(expression(beta[0])) +
  ylab("Count")

ggplot() +
  aes(b1) +
  geom_histogram(bins = 25) +
  theme_half_open() +
  ggtitle(expression("Bootstrap Distribution for" ~ beta[1])) +
  xlab(expression(beta[1])) +
  ylab("Count")
```

Estimate the standard error for each of your β coefficients as the standard deviation of the sampling distribution from your bootstrap.

```{r}
print(paste("Standard error for β0:", round(sd(b0), 3)))
print(paste("Standard error for β1:", round(sd(b1), 3)))
```

Also determine the 95% CI for each of your β coefficients based on the appropriate quantiles from your sampling distribution.
```{r}
print(paste0(
  "95% CI for β0: [", round(quantile(b0, 0.025), 3), ", ", round(quantile(b0, 0.975), 3), "]" 
))

print(paste0(
  "95% CI for β1: [", round(quantile(b1, 0.025), 3), ", ", round(quantile(b1, 0.975), 3), "]" 
))
```
  
How do the SEs estimated from the bootstrap sampling distribution compare to those estimated mathematically as part of lm() function?
```{r}
tidy(model_c2)$std.error
```
*The SEs estimated from the bootstrap are slightly higher.*

How do your bootstrap CIs compare to those estimated mathematically as part of the lm() function?
```{r}
# lm CI
confint(model_c2, level = .95)
```
*The confidence intervals for both estimates are wider for the bootstrap estimate than the lm() function.*

##CHALLENGE 3:
Write your own function, called boot_lm(), that takes as its arguments a dataframe (d=), a linear model (model=, written as a character string, e.g., “logGS ~ logBM”), a user-defined confidence interval level (conf.level=, with default “0.95”), and a number of bootstrap replicates (reps=, with default “1000”).

Your function should return a dataframe that includes: the β coefficient names (β0, β1, etc.); the value of the β coefficients, their standard errors, and their upper and lower CI limits for the linear model based on your original dataset; and the mean β coefficient estimates, SEs, and CI limits for those coefficients based on your bootstrap.

```{r}
boot_lm <- function(d, model, conf.level = 0.95, reps = 1000) {
  
  # run linear model
  m <- lm(model, data = d)
  m_stats <- tidy(m) %>%
    select(estimate, std.error)
  colnames(m_stats) <- c("Lm_Est", "Lm_SE")
  
  num_b <- nrow(m_stats)
  
  # calculate confidence interval from lm
  m_ci <- data.frame(confint(m, level = conf.level))
  colnames(m_ci) <- c("LmCI_lower", "LmCI_upper")
  
  # run bootstrap
  boot <- data.frame(matrix(nrow = reps, ncol = num_b))

  for (i in 1:reps) {
    
    slice <- d %>%
      slice_sample(n = nrow(d), replace = TRUE)
    
    lm <- lm(model, data=slice)
    
    # store coefficients
    for (j in 1:num_b) {
      boot[i, j] <- coef(lm)[[j]]
    }
    
  }
  
  # calculate bootstrap statistics
  boot_stats <- data.frame(matrix(nrow = num_b, ncol = 4))
  colnames(boot_stats) <- c("Boot_Est", "Boot_SE", "BootCI_lower", "BootCI_upper")
  for (k in 1:num_b) {
    boot_stats[k, 1] <- mean(boot[,k])
    boot_stats[k, 2] <- sd(boot[,k])
    boot_stats[k, 3] <- quantile(boot[,k], 0.025)
    boot_stats[k, 4] <- quantile(boot[,k], 0.975)
  }
  
  
  # assemble output
  outpt <- cbind(m_stats, m_ci, boot_stats)
  # return output
  return(outpt)
}
```

Use your function to run the following models on the “KamilarAndCooperData.csv” dataset:

log(MeanGroupSize) ~ log(Body_mass_female_mean)
log(DayLength_km) ~ log(Body_mass_female_mean)
log(DayLength_km) ~ log(Body_mass_female_mean) + log(MeanGroupSize)

```{r}
(outpt1 <- boot_lm(kc, "log(MeanGroupSize) ~ log(Body_mass_female_mean)"))
(outpt2 <- boot_lm(kc, "log(DayLength_km) ~ log(Body_mass_female_mean)"))
(outpt3 <- boot_lm(kc, "log(DayLength_km) ~ log(Body_mass_female_mean) + log(MeanGroupSize)"))
```

##EXTRA CREDIT:
Using a loop, run your function from Challenge 3 to generate mean β1 coefficents and lower and upper limits for the 95% CI for the log(MeanGroupSize) ~ log(Body_mass_female_mean) model with different numbers of bootstrap replicates from 10 to 200 by 5s and plot these using {ggplot2}. That is, plot these as three LINES, with the number of replicates as the x aesthetic and the mean, lower CI limit, and upper CI limit from each set of replicates as the y aesthetic. Also plot as a horizontal line the β1 value from the original linear model

HINT: The mean β1 value from each set of bootstrap replicates may not differ that much from the original model!

```{r}
seq <- seq(10, 200, 5)
store <- data.frame(matrix(nrow = length(seq), ncol = 5))
colnames(store) <- c("Reps", "Boot_Est", "Boot_SE", "BootCI_lower", "BootCI_upper")
for (i in (1:length(seq))) {
  temp <- boot_lm(kc, "log(MeanGroupSize) ~ log(Body_mass_female_mean)", reps = seq[i])
  store[i, 1] <- seq[i]
  store[i, 2] <- temp[2, 5]
  store[i, 3] <- temp[2, 6]
  store[i, 4] <- temp[2, 7]
  store[i, 5] <- temp[2, 8]
}
```

```{r}
ggplot(store) +
  geom_line(aes(y = outpt1[2,1], x = Reps)) +
  geom_line(aes(Reps, Boot_Est)) +
  geom_line(aes(Reps, BootCI_lower)) +
  geom_line(aes(Reps, BootCI_upper)) +
  theme_minimal() + 
  ggtitle(expression("Bootstrap Estimates for" ~ beta[1]), subtitle = "with 95% CI") +
  ylab(expression(beta[1])) +
  xlab("Bootstrap Replicates")
```








